{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pinecone import Pinecone\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load environment variables and initialize clients\n",
    "load_dotenv('/Users/adamhunter/miniconda3/envs/ragdev/ragdev.env')\n",
    "openai_client = OpenAI(api_key=os.environ.get('OPENAI_API_KEY'))\n",
    "pc = Pinecone(api_key=os.environ.get('PINECONE_API_KEY'))\n",
    "index = pc.Index(\"3rd-party-data-v2\")\n",
    "\n",
    "# Constants\n",
    "EMBEDDING_MODEL = \"text-embedding-3-large\"\n",
    "BATCH_SIZE = 100\n",
    "JSONL_FILE_PATH = \"/Users/adamhunter/Documents/ingestionrepos/pathlabsingestion/scraping_tradedesk/api access/data/third_party_segments/third_party_data_tpzavzt.jsonl\"  # Update this path\n",
    "\n",
    "def process_jsonl_file(file_path):\n",
    "    chunks = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            chunk = create_chunk(data)\n",
    "            chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "def flatten_dict(d, parent_key='', sep='_'):\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            items.extend(flatten_dict(v, new_key, sep=sep).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "def handle_complex_values(value):\n",
    "    if value is None:\n",
    "        return \"null\"\n",
    "    if isinstance(value, dict):\n",
    "        return flatten_dict(value)\n",
    "    if isinstance(value, list):\n",
    "        return json.dumps(value)\n",
    "    return value\n",
    "\n",
    "def create_chunk(data):\n",
    "    raw_string = f\"Full Path: {data['FullPath']}, Description: {data['Description']}\"\n",
    "    chunk_id = data['ThirdPartyDataId']\n",
    "    \n",
    "    metadata = {}\n",
    "    for key, value in data.items():\n",
    "        if key not in ['FullPath', 'Description']:\n",
    "            processed_value = handle_complex_values(value)\n",
    "            if isinstance(processed_value, dict):\n",
    "                metadata.update(processed_value)\n",
    "            else:\n",
    "                metadata[key] = processed_value\n",
    "    \n",
    "    return {\n",
    "        \"id\": chunk_id,\n",
    "        \"raw_string\": raw_string,\n",
    "        \"metadata\": metadata\n",
    "    }\n",
    "\n",
    "def generate_embeddings(batch):\n",
    "    texts = [chunk['raw_string'] for chunk in batch]\n",
    "    response = openai_client.embeddings.create(\n",
    "        model=EMBEDDING_MODEL,\n",
    "        input=texts,\n",
    "        encoding_format=\"float\",\n",
    "        dimensions=256\n",
    "    )\n",
    "    return [data.embedding for data in response.data]\n",
    "\n",
    "def prepare_and_upsert(batch, embeddings):\n",
    "    upserts = [\n",
    "        {\n",
    "            \"id\": chunk['id'],\n",
    "            \"values\": embedding,\n",
    "            \"metadata\": {**chunk['metadata'], 'raw_string': chunk['raw_string']}\n",
    "        }\n",
    "        for chunk, embedding in zip(batch, embeddings)\n",
    "    ]\n",
    "    index.upsert(upserts)\n",
    "\n",
    "def embed_and_upload_chunks(chunks):\n",
    "    for i in tqdm(range(0, len(chunks), BATCH_SIZE), desc=\"Processing and uploading chunks\"):\n",
    "        batch = chunks[i:i+BATCH_SIZE]\n",
    "        embeddings = generate_embeddings(batch)\n",
    "        prepare_and_upsert(batch, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 586277\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Main execution\n",
    "chunks = process_jsonl_file(JSONL_FILE_PATH)\n",
    "print(f\"Total chunks created: {len(chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in all raw strings: 25446228\n",
      "Total cost for all raw strings: 3.3080096400000003\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def count_tokens(text):\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "total_tokens = sum(count_tokens(chunk['raw_string']) for chunk in chunks)\n",
    "print(f\"Total tokens in all raw strings: {total_tokens}\")\n",
    "total_cost = total_tokens/1000000 * 0.13\n",
    "print(f\"Total cost for embedding all raw strings: {total_cost}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '11167698|da31shth',\n",
       " 'raw_string': 'Full Path: Asia > Life Event > College Graduation, Description: None',\n",
       " 'metadata': {'ThirdPartyDataId': '11167698|da31shth',\n",
       "  'BrandId': 'da31shth',\n",
       "  'BrandName': 'Data Alliance',\n",
       "  'Name': 'College Graduation',\n",
       "  'DevicesBrowsers30DayCount': 15906000,\n",
       "  'UniqueUserCount': 8327300,\n",
       "  'UniqueUserInAppCount': 2914500,\n",
       "  'UniqueUserWebCount': 3829900,\n",
       "  'UniqueConnectedTvCount': 1582900,\n",
       "  'CPMRate': '{\"Amount\": 1.85, \"CurrencyCode\": \"USD\"}',\n",
       "  'CPMRateInAdvertiserCurrency': '{\"Amount\": 1.85, \"CurrencyCode\": \"USD\"}',\n",
       "  'PercentOfMediaCostRate': 0.16,\n",
       "  'PersonsCount': 6051700,\n",
       "  'HouseholdCount': 4038400,\n",
       "  'ReceivedIDsCount': 15906000,\n",
       "  'ActiveIDsCount': 8327300,\n",
       "  'ActiveIDsInAppCount': 2914500,\n",
       "  'ActiveIDsWebCount': 3829900,\n",
       "  'ActiveIDsConnectedTvCount': 1582900,\n",
       "  'ActivePersonsCount': 6051700,\n",
       "  'ActiveHouseholdCount': 4038400,\n",
       "  'ActiveIDsCountExpanded': 'null'}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing and uploading chunks: 100%|██████████| 5863/5863 [1:52:09<00:00,  1.15s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 256,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 585168}},\n",
      " 'total_vector_count': 585168}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embed_and_upload_chunks(chunks)\n",
    "\n",
    "# Check index stats\n",
    "print(index.describe_index_stats())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragdev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
